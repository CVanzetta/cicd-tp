# ğŸš¨ Guide : Configurer les Alertes Grafana

## Vue d'ensemble

Nous allons configurer 3 alertes pour dÃ©tecter automatiquement les problÃ¨mes :
1. Tests sautÃ©s > 0
2. Tests Ã©chouÃ©s > 0
3. Augmentation du temps d'exÃ©cution de +30%

---

## âš™ï¸ PrÃ©requis : Configurer un Contact Point

Avant de crÃ©er des alertes, configurons oÃ¹ les notifications seront envoyÃ©es.

### Ã‰tape 1 : CrÃ©er un Contact Point

1. **â˜°** â†’ **Alerting** â†’ **Contact points**
2. Cliquez sur **+ Add contact point**
3. **Name** : `Email Notifications` (ou tout autre nom)
4. **Integration** : Choisissez une option :

#### Option A : Email (recommandÃ© pour le TP)

Pour le TP, on peut simuler sans vraie config email :
- **Name** : `TP Contact Point`
- **Integration** : `Alertmanager` (par dÃ©faut, logs seulement)
- Laissez les autres champs par dÃ©faut
- Cliquez sur **Test** â†’ **Save contact point**

#### Option B : Webhook (pour logs)

- **Integration** : `Webhook`
- **URL** : `http://localhost:9464/health` (notre exporteur)
- Cliquez sur **Test** â†’ **Save contact point**

#### Option C : Configuration rÃ©elle (optionnel)

**Slack :**
- CrÃ©ez un webhook Slack
- Collez l'URL du webhook

**Discord :**
- CrÃ©ez un webhook Discord
- Collez l'URL du webhook

**Email (si SMTP configurÃ©) :**
- Configurez le serveur SMTP dans `grafana.ini`

---

## ğŸš¨ Alerte 1 : Tests SautÃ©s > 0

### Configuration

1. **â˜°** â†’ **Alerting** â†’ **Alert rules**
2. Cliquez sur **+ Create alert rule**

### Section A : Set an alert rule name

- **Rule name** : `Tests Skipped Alert`

### Section B : Set a query and alert condition

1. **Select data source** : `Prometheus`
2. **Query A** (onglet Query) :
   ```promql
   tests_skipped
   ```
3. En dessous, dans **Expressions** :
   - Cliquez sur **+ Add expression**
   - **Type** : `Threshold`
   - **Expression** : `A`
   - **Condition** : `IS ABOVE`
   - **Value** : `0`

### Section C : Set alert evaluation behavior

1. **Folder** : SÃ©lectionnez ou crÃ©ez `CI/CD Alerts`
2. **Evaluation group** : CrÃ©ez un nouveau groupe : `Test Metrics`
3. **Evaluation interval** : `10s` (Ã©value toutes les 10 secondes)
4. **Pending period** : `0s` (alerte immÃ©diate)

### Section D : Add annotations

- **Summary** : `Tests have been skipped`
- **Description** :
  ```
  {{ $values.A }} test(s) were skipped in the last test run.
  This may indicate incomplete test coverage.
  ```
- **Runbook URL** : (optionnel) Lien vers doc de troubleshooting

### Section E : Notifications

1. **Contact point** : SÃ©lectionnez celui crÃ©Ã© prÃ©cÃ©demment
2. **Custom labels** (optionnel) :
   - `severity` = `warning`
   - `team` = `qa`

### Sauvegarder

Cliquez sur **Save rule and exit**

---

## âŒ Alerte 2 : Tests Ã‰chouÃ©s > 0

### Configuration

1. **â˜°** â†’ **Alerting** â†’ **Alert rules**
2. **+ Create alert rule**

### Sections

**A. Rule name** :
```
Tests Failed Alert
```

**B. Query and condition** :
1. Query A :
   ```promql
   tests_failed
   ```
2. Expression (Threshold) :
   - Expression : `A`
   - Condition : `IS ABOVE`
   - Value : `0`

**C. Evaluation** :
- Folder : `CI/CD Alerts`
- Group : `Test Metrics`
- Interval : `10s`
- Pending : `0s`

**D. Annotations** :
- **Summary** : `Tests have failed`
- **Description** :
  ```
  ğŸš¨ CRITICAL: {{ $values.A }} test(s) failed in the last run.
  Immediate investigation required.
  ```

**E. Notifications** :
- Contact point : Celui crÃ©Ã© prÃ©cÃ©demment
- Labels :
  - `severity` = `critical`
  - `team` = `qa`

**Sauvegarder**

---

## ğŸŒ Alerte 3 : Augmentation du Temps d'ExÃ©cution (+30%)

### Configuration AvancÃ©e

Cette alerte est plus complexe car elle compare avec une valeur prÃ©cÃ©dente.

1. **â˜°** â†’ **Alerting** â†’ **Alert rules**
2. **+ Create alert rule**

### Sections

**A. Rule name** :
```
Slow Test Execution Alert
```

**B. Query and condition** :

1. **Query A** (durÃ©e actuelle) :
   ```promql
   tests_avg_duration_ms
   ```

2. **Query B** (durÃ©e 1 minute avant) :
   ```promql
   tests_avg_duration_ms offset 1m
   ```

3. **Expression C** (Math : calcul du delta en %) :
   - Type : `Math`
   - Expression : `(A - B) / B * 100`
   - Ceci calcule le pourcentage d'augmentation

4. **Expression D** (Threshold : seuil d'alerte) :
   - Type : `Threshold`
   - Expression : `C`
   - Condition : `IS ABOVE`
   - Value : `30` (30%)

### Alternative SimplifiÃ©e (recommandÃ© pour le TP)

Si la mÃ©thode ci-dessus est trop complexe, utilisez une alerte simple :

**Query A** :
```promql
tests_avg_duration_ms
```

**Threshold** :
- Condition : `IS ABOVE`
- Value : `150` (150ms, si votre durÃ©e normale est ~100ms)

Ajustez la valeur selon votre baseline.

**C. Evaluation** :
- Folder : `CI/CD Alerts`
- Group : `Test Metrics`
- Interval : `10s`
- Pending : `30s` (attend 30s avant de dÃ©clencher)

**D. Annotations** :
- **Summary** : `Test execution time has increased significantly`
- **Description** :
  ```
  âš ï¸ WARNING: Average test duration has increased by more than 30%.
  Current: {{ $values.A }}ms
  This may indicate performance degradation or resource issues.
  ```

**E. Notifications** :
- Contact point : Celui crÃ©Ã©
- Labels :
  - `severity` = `warning`
  - `team` = `performance`

**Sauvegarder**

---

## ğŸ“Š Visualiser les Alertes dans le Dashboard

### Ajouter des seuils visuels

Retournez dans votre dashboard et Ã©ditez les panels :

**Panel "Tests Failed"** :
1. Edit panel
2. Onglet **Overrides**
3. Ajoutez une rÃ¨gle : Si `tests_failed > 0`, couleur rouge

**Panel "Average Duration"** :
1. Edit panel
2. Onglet **Thresholds** (dans les options du panel)
3. Ajoutez :
   - Base : Vert
   - 130 : Orange (warning)
   - 150 : Rouge (critical)

Cliquez sur **Apply**

---

## ğŸ§ª Tester les Alertes

### ScÃ©nario de Test Complet

1. **VÃ©rifier l'Ã©tat initial** :
   ```bash
   npm test
   ```
   Toutes les alertes doivent Ãªtre **Normal** (vert)

2. **DÃ©clencher les alertes** :
   ```bash
   npm run test:trigger-alarm
   ```

3. **Attendre 10-15 secondes** (pour que Prometheus scrape)

4. **VÃ©rifier dans Grafana** :
   - **â˜°** â†’ **Alerting** â†’ **Alert rules**
   - Les alertes devraient passer en Ã©tat **Firing** ğŸ”¥

### VÃ©rification DÃ©taillÃ©e

Pour chaque alerte, cliquez dessus et vÃ©rifiez :
- **State** : Firing
- **Labels** : severity, team
- **Annotations** : Le message d'erreur
- **Timeline** : Historique des Ã©tats

---

## ğŸ“§ Notifications (Optionnel)

### Activer les Notifications RÃ©elles

#### Slack

1. CrÃ©ez une Incoming Webhook :
   - https://api.slack.com/messaging/webhooks
2. Dans Contact points, ajoutez :
   - Integration : Slack
   - Webhook URL : Collez l'URL

#### Discord

1. Dans votre serveur Discord :
   - Server Settings â†’ Integrations â†’ Webhooks
   - Create Webhook â†’ Copy URL
2. Dans Contact points :
   - Integration : Discord
   - Webhook URL : Collez l'URL

#### Email (avec SMTP)

Ã‰ditez `grafana.ini` :
```ini
[smtp]
enabled = true
host = smtp.gmail.com:587
user = your-email@gmail.com
password = your-app-password
from_address = grafana@yourdomain.com
```

RedÃ©marrez Grafana, puis :
1. Contact point â†’ Email
2. Addresses : `your-email@example.com`

---

## ğŸ”• Silencer une Alerte (Mute)

Si vous voulez temporairement dÃ©sactiver une alerte :

1. **â˜°** â†’ **Alerting** â†’ **Silences**
2. **+ Add silence**
3. **Matchers** :
   - `alertname` = `Tests Skipped Alert`
4. **Duration** : 1h, 24h, etc.
5. **Comment** : Raison du silence
6. **Create silence**

---

## ğŸ“ˆ Dashboard d'Alertes

### CrÃ©er un Panel avec l'Ã‰tat des Alertes

1. Retournez dans votre dashboard
2. **Add** â†’ **Visualization**
3. **Type** : **Alert list**
4. **Options** :
   - **Show** : Current state
   - **State filter** : All
   - **Alert name filter** : Laissez vide pour voir toutes
5. **Title** : `ğŸš¨ Active Alerts`
6. **Apply**

Ce panel affichera en temps rÃ©el toutes les alertes actives.

---

## ğŸ¯ Checklist Alertes

- [ ] Contact point crÃ©Ã©
- [ ] Alerte "Tests Skipped" crÃ©Ã©e
- [ ] Alerte "Tests Failed" crÃ©Ã©e
- [ ] Alerte "Slow Execution" crÃ©Ã©e
- [ ] Toutes les alertes dans le mÃªme groupe "Test Metrics"
- [ ] Intervals d'Ã©valuation configurÃ©s (10s)
- [ ] Annotations avec descriptions claires
- [ ] Labels severity ajoutÃ©s
- [ ] Test avec `npm run test:trigger-alarm` rÃ©ussi
- [ ] Alertes passent en Ã©tat "Firing" ğŸ”¥
- [ ] (Optionnel) Notifications reÃ§ues

---

## ğŸ” Monitoring des Alertes

### Historique des Alertes

1. **â˜°** â†’ **Alerting** â†’ **Alert rules**
2. Cliquez sur une alerte
3. Onglet **History** : Voir tous les changements d'Ã©tat

### Statistiques

Dans la page **Alert rules** :
- **Firing** : Nombre d'alertes actives
- **Pending** : Alertes en attente
- **Normal** : Alertes OK

---

## ğŸ†˜ Troubleshooting Alertes

### ProblÃ¨me : Alerte ne se dÃ©clenche jamais

**Solutions :**
1. VÃ©rifier la query Prometheus :
   - Aller sur http://localhost:9090/graph
   - Tester la query manuellement
2. VÃ©rifier le seuil (threshold) n'est pas trop haut
3. VÃ©rifier l'Ã©valuation interval (peut-Ãªtre trop long)
4. VÃ©rifier que les donnÃ©es arrivent dans Prometheus

### ProblÃ¨me : Alerte se dÃ©clenche en boucle

**Solutions :**
1. Augmenter le **Pending period** (ex: 30s au lieu de 0s)
2. Ajouter un **Recovery threshold** diffÃ©rent du seuil de dÃ©clenchement
3. Utiliser une **moyenne sur plusieurs points** avec :
   ```promql
   avg_over_time(tests_failed[1m])
   ```

### ProblÃ¨me : Pas de notification reÃ§ue

**Solutions :**
1. VÃ©rifier le Contact point est bien configurÃ©
2. Tester le contact point manuellement (bouton **Test**)
3. VÃ©rifier les logs Grafana :
   ```bash
   # Logs Windows
   C:\Program Files\GrafanaLabs\grafana\data\log\

   # Logs macOS/Linux
   /var/log/grafana/grafana.log
   ```
4. VÃ©rifier que l'alerte a bien le contact point assignÃ©

### ProblÃ¨me : Alerte reste en "Pending"

**Solution :**
- C'est normal si le pending period est > 0s
- L'alerte passe en "Firing" aprÃ¨s ce dÃ©lai
- RÃ©duire Ã  `0s` pour des alertes immÃ©diates

---

## ğŸ“š PromQL pour Alertes AvancÃ©es

### Exemples de Queries Utiles

**Taux de rÃ©ussite sous 90% :**
```promql
(tests_passed / tests_total) * 100 < 90
```

**Plus de 5 tests Ã©chouÃ©s :**
```promql
tests_failed > 5
```

**DurÃ©e en augmentation constante sur 5 min :**
```promql
deriv(tests_avg_duration_ms[5m]) > 0
```

**Aucun test exÃ©cutÃ© depuis 1h :**
```promql
time() - timestamp(tests_total) > 3600
```

---

## ğŸ“ Best Practices

### Bonnes Pratiques d'Alerting

1. **Utilisez des labels** : Pour filtrer et router les alertes
2. **Descriptions claires** : Incluez des Ã©tapes de rÃ©solution
3. **Ã‰vitez le spam** : Utilisez pending period et thresholds adaptÃ©s
4. **Testez rÃ©guliÃ¨rement** : VÃ©rifiez que les alertes fonctionnent
5. **Documentez** : Liens vers runbooks dans les annotations
6. **SÃ©vÃ©ritÃ© appropriÃ©e** : Critical, Warning, Info
7. **On-call rotation** : Assignez des Ã©quipes responsables

### Structure de Labels RecommandÃ©e

```yaml
severity: critical | warning | info
team: qa | dev | ops
service: tests | build | deploy
environment: prod | staging | dev
```

---

## ğŸ“Š Exemple de RÃ¨gles Multiples

Pour crÃ©er un systÃ¨me d'alerting complet :

```yaml
# Critique : Tests Ã©chouÃ©s
tests_failed > 0
â†’ severity: critical, team: qa

# Warning : Tests lents
tests_avg_duration_ms > 150
â†’ severity: warning, team: performance

# Info : Nouveau run de tests
changes(tests_total[1m]) > 0
â†’ severity: info, team: qa
```

---

**âœ… Alertes configurÃ©es ! Testez avec `npm run test:trigger-alarm` ğŸ”¥**
